{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LinearRegression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPte6N+Gq+MA8UKDIDoOagM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ilovely11/ly11/blob/master/LinearRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJcOQ5oyQ690",
        "colab_type": "text"
      },
      "source": [
        "# **Housing in California**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_f_er9JTGFJE",
        "colab_type": "text"
      },
      "source": [
        "This dataset came with Google Colab, so we tried to do some anaylsis on it.  We'll try Linear Regression, KNN, and K-means. \n",
        "\n",
        "In this tuotiral, we used a guide from the machine learning course taught by Guowei Wei at MSU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1QyUa32G0Kq",
        "colab_type": "text"
      },
      "source": [
        "# About the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-xVD4OnG5DV",
        "colab_type": "text"
      },
      "source": [
        "This dataset is from 1990 California Census data, which originally appeared in it's current form in a 1997 paper titled \"Sparse Spatial Autoregressions\" by R. Kelley Pace and Ronald Barry. The variables in this dataset are longitude, latitutde, median housing price, total rooms, total bedrooms, population per block, household income (in units of 10,000 USD), ocean proximity, and median house value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGsQgrZ3Uqea",
        "colab_type": "text"
      },
      "source": [
        "#Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZkK4I36Un5B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib as plt\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFvaLieibcMG",
        "colab_type": "text"
      },
      "source": [
        "#Putting Our Data Into a Pandas Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXNvhpcDDa7D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = pd.read_csv('/content/sample_data/california_housing_train.csv')\n",
        "\n",
        "X_train_dataframe = train_data.drop(columns = ['longitude', 'latitude' ,'median_house_value'])\n",
        "X_train = X_train_dataframe.values\n",
        "\n",
        "y_train_dataframe = train_data['median_house_value']\n",
        "y_train = y_train_dataframe.values\n",
        "\n",
        "test_data = pd.read_csv('/content/sample_data/california_housing_test.csv')\n",
        "\n",
        "X_test_dataframe = test_data.drop(columns = ['longitude', 'latitude' ,'median_house_value'])\n",
        "X_test = X_test_dataframe.values\n",
        "\n",
        "y_test_dataframe = test_data['median_house_value']\n",
        "y_test = y_test_dataframe.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkivzkhrQ2Mc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaler = StandardScaler() # call an object function\n",
        "scaler.fit(X_train)   # calculate mean\n",
        "X_train_norm = scaler.transform(X_train)  # apply normalization on X_train\n",
        "X_test_norm = scaler.transform(X_test)    # apply normalization on X_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SltlObhjJjXk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_norm, X_test_norm = normalize_features(X_train, X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHozHSvRJ9yq",
        "colab_type": "code",
        "outputId": "de33a2ad-74a6-4246-a0c3-c52970df10c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(X_train_norm.shape)\n",
        "print(X_test_norm.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(17000, 6)\n",
            "(3000, 6)\n",
            "(17000,)\n",
            "(3000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fjj0hTjHNzzo",
        "colab_type": "text"
      },
      "source": [
        "# The Theory of Regression With Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7hFDeTQKG_A",
        "colab_type": "text"
      },
      "source": [
        "We'll start by implementing linear regression with a regularization. In particular, we'll implement a ridge regression since LASSO will take a very similar approach.\n",
        "\n",
        "We also choose a method involving regularization since we can't solve for the optimal weights using analytical methods. Instead, we'll have to implement numerical methods using gradient descent, which fits more with modern methods used for deep learning.\n",
        "\n",
        "Since this is a linear regression, we assume that our data can be reasonably modeled by a straight line in some multidimensional space, let's say\n",
        "$$\\hat{y} = XW + b$$\n",
        "In this case, $\\hat{y}$ is our predicted output, which is the median housing price in a block. The vector $b$ represents the bias. This tell us the difference between the true values and the predicted values in our model. This will be a vector where the value of each entry is the same since it affects our whole dataset globally.\n",
        "\n",
        "Assume our training set has $N$ datapoints and we have $D$ features. Then let $X$ be the matrix which contains each feature as the column entries and each datapoints is the row. The vector $W$ is the weight associated with each feature. Thus, $X$ has shape $(N,D)$ and  $W$ has shape $(D,1)$, \n",
        "\n",
        "We want the set of weights and bias, $W$ and $b$, that allows us to make the best prediction on our test data. However, how can we measure what is the best fit? We'll check the squared different between our predicted housing prices in a block and our actual housing prices in a block, which we'll call $y$. That is, our loss function will be:\n",
        "$$L = \\frac{\\|\\hat{y} - y\\|_2^2}{2N} = \\frac{1}{2N} \\sum_{i = 1}^{N} (\\hat{y}_i - y_i)^2.$$\n",
        "To account for overfitting, we'll introduce a regularization parameter, $\\alpha$ and let our new loss function be\n",
        "$$L = \\frac{\\|\\hat{y} - y\\|_2^2}{2N} + \\frac{\\alpha}{2N} ||W||_2^2 = \\frac{1}{2N} \\sum_{i = 1}^N (\\hat{y}_i - y_i)^2 + \\frac{\\alpha}{2N} \\sum_{n=1}^{N} ||W_n||_2^2$$\n",
        "For simplicity, we'll code this in terms of the vector versions rather than working element by element.\n",
        "\n",
        "We're implementing this with gradient descent. Our unknown parameters we want to find are the matrix $W$ and the value $b$, so we'll find the partial derivatives of our loss function with respect to the variables for use in the back propagation. First, let's start with $W$:\n",
        "$$\\dfrac{\\partial{L}}{\\partial{W}} = \\dfrac{\\partial{L}}{\\partial{\\hat{y}}}\\dfrac{\\partial{\\hat{y}}}{\\partial{W}} = \\frac{1}{N} X^T (\\hat{y}-y) + \\frac{\\alpha}{N}W.$$\n",
        "Next, let's do this with $b$:\n",
        "$$\\dfrac{\\partial{L}}{\\partial{W}} = \\dfrac{\\partial{L}}{\\partial{\\hat{y}}}\\dfrac{\\partial{\\hat{y}}}{\\partial{W}} = \\frac{1}{N} X^T (\\hat{y}-y) + \\frac{\\alpha}{N}W.$$\n",
        "Since we're implementing gradient descent, we need a rate at which our algorithm \"learns,\" which we'll call $\\beta$. In more mathematical terms, this is just a scaling factor on our parial derivative that we tweak to find our minimum. We'll update our weights and bias in the following way:\n",
        "\\begin{align*}\n",
        "  W_{new} &= W_{old} - \\beta\\dfrac{\\partial{L}}{\\partial{W}} = W_{old} - \\frac{\\beta}{N} X^T (\\hat{y}-y) - \\frac{\\beta\\alpha}{N}W, \\\\\n",
        "  b_{new} &= b_{old} - \\beta\\dfrac{\\partial{L}}{\\partial{b}} = b_{old} - \\frac{\\beta}{N} \\sum_{n=1}^{N}\\hat{y}_n - y_n.\n",
        "\\end{align*}\n",
        "This is starting to look more like something we can code up now!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GFvUVsvknQh",
        "colab_type": "text"
      },
      "source": [
        "# Forward Propagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ref3yx6Hkrmt",
        "colab_type": "text"
      },
      "source": [
        "This is the easy part! We'll start by calculating what our prediction would be:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-u222xWkNI4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward(X, W, b):\n",
        "    y_hat = np.dot(X, W) + b\n",
        "    return y_hat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0u3-pzTk665",
        "colab_type": "text"
      },
      "source": [
        "# Backward Propagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-XY1Bcpk_qJ",
        "colab_type": "text"
      },
      "source": [
        "So, our initial guess probably isn't the minimzing guess, so what we'll do is update our guess for $W$ and $b$ by using gradient descent:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EULdTBRDlMeJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gradient_descent(W, b, y, y_hat, X, alpha, beta):\n",
        "    X_T = np.transpose(X)\n",
        "    z = (y_hat - y)/X.shape[0]\n",
        "    grad_W = np.matmul(X_T, z)  + alpha / X.shape[0] * W\n",
        "    grad_b = np.sum(z, axis = 0, keepdims = True)\n",
        "    W = W - beta * grad_W\n",
        "    b = b - beta * grad_b\n",
        "    return W, b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5Z12s8DmKoR",
        "colab_type": "text"
      },
      "source": [
        "# Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUlNw_hNmPDL",
        "colab_type": "text"
      },
      "source": [
        "Now that we have all the pieces for our algorithm, what's left is to predict after running through enough iterations. We'll take our test data and assume it fits the model. Then we have\n",
        "$$y_{predicted} = X_{test}W_{optimal} + b_{optimal}.$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGm_fkf-mGWT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(W, b, X_test):\n",
        "    y_pred = np.dot(X_test, W) + b\n",
        "    return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LVh74wchuM8",
        "colab_type": "text"
      },
      "source": [
        "# Checking the Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rt2IlOf7hxJD",
        "colab_type": "text"
      },
      "source": [
        "We need to check if our code is actually minimizing our loss, so let's write a function print out the loss:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGiPmnOeh3QX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_loss(y_hat, y):\n",
        "    return np.linalg.norm(y_hat-y)**2/len(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0uyn092m2_n",
        "colab_type": "text"
      },
      "source": [
        "# Putting it Together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-IAESbWm8Ic",
        "colab_type": "text"
      },
      "source": [
        "Okay, so now that we have all the required pieces, let's put it all together. As with any numerical algorithm, let's put this through multiple iterations in a loop and then predict:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlVYFvKnnIwt",
        "colab_type": "code",
        "outputId": "9d8989b9-6cb2-4944-e904-7be95e650fe0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "alpha = 10\n",
        "beta = 0.01\n",
        "W = np.zeros((X_train_norm.shape[1],1), dtype = 'float')\n",
        "y_hat = np.zeros((X_train_norm.shape[0],1), dtype = 'float')\n",
        "b = 0.0\n",
        "iterations = 2000 #this will probably take a long time without a GPU\n",
        "for n in range(iterations):\n",
        "    y_hat = forward(X_train_norm, W, b)\n",
        "    W, b = gradient_descent(W, b, y_train, y_hat, X_train_norm, alpha, beta)\n",
        "    loss = print_loss(y_hat, y_train)\n",
        "    print(loss)\n",
        "pred = predict(W, X_test_norm, b)\n",
        "pred = np.matrix.round(pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "959226879358459.5\n",
            "940138264564950.5\n",
            "921429513079511.6\n",
            "903093065761407.4\n",
            "885121513775828.1\n",
            "867507595634199.0\n",
            "850244194489207.8\n",
            "833324335015866.1\n",
            "816741180750209.0\n",
            "800488031257281.1\n",
            "784558319437753.2\n",
            "768945608881602.5\n",
            "753643591266581.8\n",
            "738646083785866.6\n",
            "723947026721624.5\n",
            "709540480892980.4\n",
            "695420625321365.0\n",
            "681581754877437.9\n",
            "668018277953756.0\n",
            "654724714224768.1\n",
            "641695692411971.5\n",
            "628925948135713.0\n",
            "616410321766313.1\n",
            "604143756364315.8\n",
            "592121295603900.1\n",
            "580338081831890.8\n",
            "568789354001768.6\n",
            "557470445860087.2\n",
            "546376783977920.25\n",
            "535503885975740.44\n",
            "524847358649267.6\n",
            "514402896216646.5\n",
            "504166278581246.44\n",
            "494133369637677.8\n",
            "484300115583121.8\n",
            "474662543282999.44\n",
            "465216758666393.9\n",
            "455958945174373.7\n",
            "446885362162316.44\n",
            "437992343452183.8\n",
            "429276295821726.25\n",
            "420733697534023.6\n",
            "412361096948407.6\n",
            "404155111122099.0\n",
            "396112424409858.1\n",
            "388229787161634.5\n",
            "380504014403449.2\n",
            "372931984512950.94\n",
            "365510638023655.06\n",
            "358236976329723.8\n",
            "351108060496937.1\n",
            "344121010096329.5\n",
            "337273001991797.25\n",
            "330561269253848.44\n",
            "323983099996887.94\n",
            "317535836305355.1\n",
            "311216873162936.4\n",
            "305023657388327.06\n",
            "298953686602495.6\n",
            "293004508238906.44\n",
            "287173718529831.75\n",
            "281458961531310.88\n",
            "275857928193002.62\n",
            "270368355420473.53\n",
            "264988025152353.28\n",
            "259714763448915.94\n",
            "254546439658240.25\n",
            "249480965505434.34\n",
            "244516294297004.22\n",
            "239650420037713.62\n",
            "234881376682284.8\n",
            "230207237282164.84\n",
            "225626113261999.78\n",
            "221136153608283.56\n",
            "216735544150678.53\n",
            "212422506821270.56\n",
            "208195298938188.53\n",
            "204052212486319.4\n",
            "199991573458435.75\n",
            "196011741146517.06\n",
            "192111107498969.44\n",
            "188288096459899.78\n",
            "184541163338756.8\n",
            "180868794189565.94\n",
            "177269505184177.34\n",
            "173741842030470.22\n",
            "170284379377112.28\n",
            "166895720225751.3\n",
            "163574495392026.44\n",
            "160319362933629.94\n",
            "157129007612180.28\n",
            "154002140361041.12\n",
            "150937497767559.94\n",
            "147933841561789.5\n",
            "144989958114385.06\n",
            "142104657948239.6\n",
            "139276775256004.77\n",
            "136505167429087.08\n",
            "133788714597071.6\n",
            "131126319174319.94\n",
            "128516905424994.62\n",
            "125959419004942.8\n",
            "123452826568776.89\n",
            "120996115318691.97\n",
            "118588292623091.69\n",
            "116228385600745.58\n",
            "113915440726302.61\n",
            "111648523457186.19\n",
            "109426717840136.31\n",
            "107249126154942.78\n",
            "105114868544801.64\n",
            "103023082660199.67\n",
            "100972923316357.89\n",
            "98963562140938.28\n",
            "96994187256457.02\n",
            "95064002928246.2\n",
            "93172229270101.12\n",
            "91318101908292.33\n",
            "89500871679657.42\n",
            "87719804334060.53\n",
            "85974180227014.06\n",
            "84263294041168.25\n",
            "82586454488857.17\n",
            "80942984044929.06\n",
            "79332218663272.47\n",
            "77753507511246.23\n",
            "76206212711667.33\n",
            "74689709078433.73\n",
            "73203383868196.06\n",
            "71746636529152.11\n",
            "70318878461595.86\n",
            "68919532780005.68\n",
            "67548034077918.59\n",
            "66203828199995.2\n",
            "64886372018678.39\n",
            "63595133216336.555\n",
            "62329590065065.36\n",
            "61089231222869.25\n",
            "59873555520826.78\n",
            "58682071766204.53\n",
            "57514298538550.484\n",
            "56369763997685.5\n",
            "55248005693596.49\n",
            "54148570380106.234\n",
            "53071013830069.125\n",
            "52014900654765.07\n",
            "50979804131228.78\n",
            "49965306029744.1\n",
            "48970996439715.5\n",
            "47996473610081.49\n",
            "47041343785570.234\n",
            "46105221044229.67\n",
            "45187727145233.84\n",
            "44288491375011.055\n",
            "43407150396804.33\n",
            "42543348103451.53\n",
            "41696735476913.29\n",
            "40866970440303.98\n",
            "40053717728799.88\n",
            "39256648746089.34\n",
            "38475441435974.45\n",
            "37709780151837.445\n",
            "36959355526443.13\n",
            "36223864351675.664\n",
            "35503009450675.84\n",
            "34796499562986.65\n",
            "34104049221426.613\n",
            "33425378642035.727\n",
            "32760213606751.4\n",
            "32108285356557.504\n",
            "31469330477591.668\n",
            "30843090801212.664\n",
            "30229313293914.65\n",
            "29627749959581.543\n",
            "29038157735706.2\n",
            "28460298396290.195\n",
            "27893938458645.617\n",
            "27338849083285.973\n",
            "26794805986156.836\n",
            "26261589346881.305\n",
            "25738983719096.207\n",
            "25226777943411.496\n",
            "24724765061967.656\n",
            "24232742237114.97\n",
            "23750510667056.773\n",
            "23277875504693.395\n",
            "22814645781972.67\n",
            "22360634331115.68\n",
            "21915657707811.367\n",
            "21479536119336.95\n",
            "21052093350756.934\n",
            "20633156692955.754\n",
            "20222556874825.11\n",
            "19820127993004.05\n",
            "19425707445941.36\n",
            "19039135867667.84\n",
            "18660257063989.336\n",
            "18288917948352.96\n",
            "17924968481369.496\n",
            "17568261608413.678\n",
            "17218653202339.938\n",
            "16876002003881.986\n",
            "16540169563970.348\n",
            "16211020189526.934\n",
            "15888420887800.545\n",
            "15572241312243.84\n",
            "15262353710075.945\n",
            "14958632871070.197\n",
            "14660956077123.074\n",
            "14369203050887.268\n",
            "14083255910243.596\n",
            "13802999117819.922\n",
            "13528319435237.686\n",
            "13259105878593.604\n",
            "12995249671681.758\n",
            "12736644203053.99\n",
            "12483184983529.92\n",
            "12234769602268.502\n",
            "11991297687200.482\n",
            "11752670863123.203\n",
            "11518792712973.918\n",
            "11289568738159.309\n",
            "11064906320197.018\n",
            "10844714684368.447\n",
            "10628904862162.58\n",
            "10417389655364.678\n",
            "10210083601258.074\n",
            "10006902937677.54\n",
            "9807765569155.775\n",
            "9612591034294.656\n",
            "9421300472721.766\n",
            "9233816593410.402\n",
            "9050063643179.275\n",
            "8869967376578.95\n",
            "8693455025745.137\n",
            "8520455270809.877\n",
            "8350898210986.299\n",
            "8184715336473.003\n",
            "8021839501377.616\n",
            "7862204895144.903\n",
            "7705747017869.599\n",
            "7552402652247.1\n",
            "7402109839347.131\n",
            "7254807853585.537\n",
            "7110437177272.513\n",
            "6968939477445.657\n",
            "6830257581957.034\n",
            "6694335456054.263\n",
            "6561118180455.389\n",
            "6430551928603.595\n",
            "6302583945210.61\n",
            "6177162524716.081\n",
            "6054236990483.463\n",
            "5933757674377.178\n",
            "5815675896682.545\n",
            "5699943946319.428\n",
            "5586515061795.944\n",
            "5475343412067.287\n",
            "5366384078149.679\n",
            "5259593034985.769\n",
            "5154927133613.993\n",
            "5052344083670.982\n",
            "4951802436370.908\n",
            "4853261567941.693\n",
            "4756681662720.528\n",
            "4662023697652.303\n",
            "4569249426016.714\n",
            "4478321362433.127\n",
            "4389202767374.7944\n",
            "4301857632292.3022\n",
            "4216250665363.445\n",
            "4132347277129.2783\n",
            "4050113566291.5137\n",
            "3969516306420.848\n",
            "3890522931884.314\n",
            "3813101525487.733\n",
            "3737220805150.771\n",
            "3662850111144.838\n",
            "3589959393933.777\n",
            "3518519202024.9937\n",
            "3448500669886.566\n",
            "3379875506512.1265\n",
            "3312615983950.5786\n",
            "3246694925869.115\n",
            "3182085696860.646\n",
            "3118762191506.64\n",
            "3056698823918.654\n",
            "2995870517265.925\n",
            "2936252693983.7124\n",
            "2877821265361.52\n",
            "2820552622221.4033\n",
            "2764423624987.37\n",
            "2709411594892.759\n",
            "2655494304145.563\n",
            "2602649967487.0093\n",
            "2550857233130.62\n",
            "2500095174198.058\n",
            "2450343280240.822\n",
            "2401581448956.96\n",
            "2353789978107.218\n",
            "2306949557554.757\n",
            "2261041261361.997\n",
            "2216046540231.8374\n",
            "2171947214112.4263\n",
            "2128725464549.5444\n",
            "2086363827798.6\n",
            "2044845187604.246\n",
            "2004152768395.9136\n",
            "1964270128287.4128\n",
            "1925181152739.0894\n",
            "1886870047811.8484\n",
            "1849321333889.0735\n",
            "1812519839337.9734\n",
            "1776450694497.3376\n",
            "1741099325696.272\n",
            "1706451449098.6824\n",
            "1672493065289.0232\n",
            "1639210453253.9253\n",
            "1606590165246.7385\n",
            "1574619020960.9421\n",
            "1543284102439.831\n",
            "1512572748833.4377\n",
            "1482472551101.3728\n",
            "1452971347343.0012\n",
            "1424057217540.1523\n",
            "1395718478909.5752\n",
            "1367943681178.8115\n",
            "1340721601900.639\n",
            "1314041242053.8606\n",
            "1287891821322.5972\n",
            "1262262774087.274\n",
            "1237143744869.9875\n",
            "1212524584342.8652\n",
            "1188395345113.254\n",
            "1164746277741.1724\n",
            "1141567826836.4976\n",
            "1118850627063.0227\n",
            "1096585499581.08\n",
            "1074763448155.9097\n",
            "1053375655538.5203\n",
            "1032413479972.3455\n",
            "1011868451739.0442\n",
            "991732269539.6666\n",
            "971996797380.9177\n",
            "952654061114.017\n",
            "933696245290.8563\n",
            "915115690025.9916\n",
            "896904887793.8085\n",
            "879056480522.8512\n",
            "861563256559.646\n",
            "844418147752.327\n",
            "827614226602.9596\n",
            "811144703499.9463\n",
            "795002923896.1956\n",
            "779182365710.3685\n",
            "763676636645.1707\n",
            "748479471561.1376\n",
            "733584730080.8334\n",
            "718986393954.7102\n",
            "704678564716.3809\n",
            "690655461271.9384\n",
            "676911417594.7809\n",
            "663440880385.8802\n",
            "650238406872.6796\n",
            "637298662570.9845\n",
            "624616419191.2266\n",
            "612186552447.3556\n",
            "600004040047.8185\n",
            "588063959654.0227\n",
            "576361486855.9249\n",
            "564891893267.1453\n",
            "553650544596.2451\n",
            "542632898757.382\n",
            "531834504064.82306\n",
            "521250997434.15497\n",
            "510878102589.245\n",
            "500711628351.24854\n",
            "490747466950.6753\n",
            "480981592353.17725\n",
            "471410058662.6451\n",
            "462028998499.7666\n",
            "452834621428.1377\n",
            "443823212456.3473\n",
            "434991130534.7393\n",
            "426334807038.0745\n",
            "417850744374.83\n",
            "409535514559.8415\n",
            "401385757822.0272\n",
            "393398181243.7356\n",
            "385569557438.929\n",
            "377896723243.3108\n",
            "370376578445.1931\n",
            "363006084542.457\n",
            "355782263458.7648\n",
            "348702196415.4727\n",
            "341763022704.17725\n",
            "334961938553.5413\n",
            "328296195975.0619\n",
            "321763101675.58405\n",
            "315360015955.3156\n",
            "309084351633.00836\n",
            "302933573039.7746\n",
            "296905194936.02155\n",
            "290996781554.04193\n",
            "285205945599.99664\n",
            "279530347285.0905\n",
            "273967693375.31772\n",
            "268515736275.03348\n",
            "263172273121.83078\n",
            "257935144888.24826\n",
            "252802235506.14465\n",
            "247771471019.20212\n",
            "242840818743.0217\n",
            "238008286450.7682\n",
            "233271921553.61792\n",
            "228629810311.21228\n",
            "224080077087.83197\n",
            "219620883554.73776\n",
            "215250427968.08365\n",
            "210966944452.079\n",
            "206768702261.35513\n",
            "202654005084.6215\n",
            "198621190384.69583\n",
            "194668628695.25635\n",
            "190794722984.0705\n",
            "186997907996.32495\n",
            "183276649628.4612\n",
            "179629444300.5773\n",
            "176054818356.36496\n",
            "172551327471.54346\n",
            "169117556055.50223\n",
            "165752116690.5676\n",
            "162453649568.16086\n",
            "159220821940.381\n",
            "156052327584.95206\n",
            "152946886267.2915\n",
            "149903243229.98392\n",
            "146920168689.40067\n",
            "143996457331.542\n",
            "141130927831.47028\n",
            "138322422367.47842\n",
            "135569806161.22618\n",
            "132871967019.28517\n",
            "130227814876.27705\n",
            "127636281360.71162\n",
            "125096319360.56953\n",
            "122606902606.43706\n",
            "120167025244.21661\n",
            "117775701442.60739\n",
            "115431964982.54225\n",
            "113134868878.79657\n",
            "110883484988.21896\n",
            "108676903637.75021\n",
            "106514233254.69481\n",
            "104394600013.68262\n",
            "102317147472.51279\n",
            "100281036239.03717\n",
            "98285443617.56517\n",
            "96329563290.12863\n",
            "94412604980.55045\n",
            "92533794141.34479\n",
            "90692371638.10942\n",
            "88887593442.3693\n",
            "87118730332.51224\n",
            "85385067599.40794\n",
            "83685904753.43567\n",
            "82020555249.22206\n",
            "80388346199.40762\n",
            "78788618110.5063\n",
            "77220724610.64226\n",
            "75684032189.6761\n",
            "74177919949.27994\n",
            "72701779342.77155\n",
            "71255013934.50294\n",
            "69837039155.63118\n",
            "68447282077.535484\n",
            "67085181163.776985\n",
            "65750186058.9396\n",
            "64441757356.74306\n",
            "63159366384.18151\n",
            "61902494993.1047\n",
            "60670635342.72499\n",
            "59463289699.57507\n",
            "58279970235.21279\n",
            "57120198826.794464\n",
            "55983506870.07892\n",
            "54869435083.83829\n",
            "53777533326.32794\n",
            "52707360412.959206\n",
            "51658483940.08001\n",
            "50630480109.73207\n",
            "49622933555.22628\n",
            "48635437177.4817\n",
            "47667591977.99175\n",
            "46719006897.74707\n",
            "45789298660.949295\n",
            "44878091616.91892\n",
            "43985017594.29191\n",
            "43109715743.90562\n",
            "42251832400.61302\n",
            "41411020935.496216\n",
            "40586941618.77182\n",
            "39779261481.076096\n",
            "38987654177.42535\n",
            "38211799859.08753\n",
            "37451385041.82608\n",
            "36706102479.71555\n",
            "35975651040.21719\n",
            "35259735584.79229\n",
            "34558066846.75329\n",
            "33870361316.295616\n",
            "33196341126.549458\n",
            "32535733937.857914\n",
            "31888272832.523018\n",
            "31253696202.573547\n",
            "30631747648.37911\n",
            "30022175870.644825\n",
            "29424734570.457478\n",
            "28839182352.394646\n",
            "28265282623.565624\n",
            "27702803499.44216\n",
            "27151517709.692814\n",
            "26611202507.28212\n",
            "26081639577.671093\n",
            "25562614949.727444\n",
            "25053918912.658905\n",
            "24555345926.113434\n",
            "24066694542.109318\n",
            "23587767320.84271\n",
            "23118370751.041466\n",
            "22658315172.936516\n",
            "22207414701.34369\n",
            "21765487148.86509\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3XowRgVn_AG",
        "colab_type": "text"
      },
      "source": [
        "How accurate is this now? Let's check:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ykpun4rnoGPU",
        "colab_type": "code",
        "outputId": "b9b64eb7-2fa0-4667-f09d-75fbc462801c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(y_test)\n",
        "print (pred)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[344700. 176500. 270500. ...  62000. 162500. 500001.]\n",
            "[329914.05177176 212858.80609712 271123.15939573 ...  85871.18395124\n",
            " 183840.49451409 436313.22181434]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3TouMedo5jT",
        "colab_type": "text"
      },
      "source": [
        "Not bad for linear regression. We could probably feature engineer this to make it a little better, but we probably won't get anything that accurate. We've erased any variables about location since it definitely isn't a variable that scales linearly.  However, location is definitely an important part of housing price. This problem probably won't be solved well using regression, so let's try something a little different now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6b7Sc0NcwE6a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}