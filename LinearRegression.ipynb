{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LinearRegression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyObqRXACf9LsEG0MFilUESP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ilovely11/ly11/blob/master/LinearRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJcOQ5oyQ690",
        "colab_type": "text"
      },
      "source": [
        "# **Housing in California**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_f_er9JTGFJE",
        "colab_type": "text"
      },
      "source": [
        "The point of this notebook is to create a Linear Regression algorithm from scratch. In this notebook, we used a guide from the machine learning course taught by Guowei Wei at MSU, but our derivation of backpropagating is a little more involved mathematically. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1QyUa32G0Kq",
        "colab_type": "text"
      },
      "source": [
        "# About the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-xVD4OnG5DV",
        "colab_type": "text"
      },
      "source": [
        "This dataset is from 1990 California Census data, which originally appeared in it's current form in a 1997 paper titled \"Sparse Spatial Autoregressions\" by R. Kelley Pace and Ronald Barry. The variables in this dataset are longitude, latitutde, median housing price, total rooms, total bedrooms, population per block, household income (in units of 10,000 USD), ocean proximity, and median house value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGsQgrZ3Uqea",
        "colab_type": "text"
      },
      "source": [
        "#Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZkK4I36Un5B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib as plt\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFvaLieibcMG",
        "colab_type": "text"
      },
      "source": [
        "#Putting Our Data Into a Pandas Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXNvhpcDDa7D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = pd.read_csv('/content/sample_data/california_housing_train.csv')\n",
        "\n",
        "X_train_dataframe = train_data.drop(columns = ['longitude', 'latitude' ,'median_house_value'])\n",
        "X_train = X_train_dataframe.values\n",
        "\n",
        "y_train_dataframe = train_data['median_house_value']\n",
        "y_train = y_train_dataframe.values\n",
        "\n",
        "test_data = pd.read_csv('/content/sample_data/california_housing_test.csv')\n",
        "\n",
        "X_test_dataframe = test_data.drop(columns = ['longitude', 'latitude' ,'median_house_value'])\n",
        "X_test = X_test_dataframe.values\n",
        "\n",
        "y_test_dataframe = test_data['median_house_value']\n",
        "y_test = y_test_dataframe.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkivzkhrQ2Mc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaler = StandardScaler() # call an object function\n",
        "scaler.fit(X_train)   # calculate mean\n",
        "X_train_norm = scaler.transform(X_train)  # apply normalization on X_train\n",
        "X_test_norm = scaler.transform(X_test)    # apply normalization on X_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHozHSvRJ9yq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "4a27b0b2-6d0f-4ab3-f230-3d1d9c9327c2"
      },
      "source": [
        "print(X_train_norm.shape)\n",
        "print(X_test_norm.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(17000, 6)\n",
            "(3000, 6)\n",
            "(17000,)\n",
            "(3000,)\n",
            "[ 1.36169494  2.29660752 -0.88246225 ...  0.01529238  0.01299867\n",
            " -0.377848  ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fjj0hTjHNzzo",
        "colab_type": "text"
      },
      "source": [
        "# The Theory of Regression With Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7hFDeTQKG_A",
        "colab_type": "text"
      },
      "source": [
        "We'll start by implementing linear regression with a regularization. In particular, we'll implement a ridge regression since LASSO will take a very similar approach.\n",
        "\n",
        "We also choose a method involving regularization since we can't solve for the optimal weights using analytical methods. Instead, we'll have to implement numerical methods using gradient descent, which fits more with modern methods used for deep learning.\n",
        "\n",
        "Since this is a linear regression, we assume that our data can be reasonably modeled by a straight line in some multidimensional space, let's say\n",
        "$$\\hat{y} = XW + b.$$\n",
        "In this case, $\\hat{y}$ is our predicted output, which is the median housing price in a block. The vector $b$ represents the bias. This tell us the difference between the true values and the predicted values in our model. This will be a vector where the value of each entry is the same since it affects our whole dataset globally.\n",
        "\n",
        "Assume our training set has $N$ datapoints and we have $D$ features. Then let $X$ be the matrix which contains each feature as the column entries and each datapoints is the row. The vector $W$ is the set of weights associated with each feature. Thus, $X$ has shape $(N,D)$ and $W$ has shape $(D,1)$, \n",
        "\n",
        "We want the set of weights and bias, $W$ and $b$, that allows us to make the best prediction on our test data. However, how can we measure what is the best fit? We'll check the squared different between our predicted housing prices in a block and our actual housing prices in a block, which we'll call $y$. That is, our loss function will be:\n",
        "$$L = \\frac{\\|\\hat{y} - y\\|_2^2}{2N} = \\frac{1}{2N} \\sum_{i = 1}^{N} (\\hat{y}_i - y_i)^2.$$\n",
        "To account for overfitting, we'll introduce a regularization parameter, $\\alpha$ and let our new loss function be\n",
        "\\begin{align*}\n",
        "L = \\frac{\\|\\hat{y} - y\\|_2^2}{2N} + \\frac{\\alpha}{2N} ||W||_2^2 &= \\frac{1}{2N} \\sum_{i = 1}^N (\\hat{y}_i - y_i)^2 + \\frac{\\alpha}{2N} \\sum_{n=1}^{N} ||w_n||_2^2 \\\\\n",
        "&= \\frac{1}{2N} \\sum_{i = 1}^N (\\hat{y}_i - y_i)^2 + \\frac{\\alpha}{2N} \\sum_{n=1}^{N} (w_1 + \\cdots + w_n)^2 \\\\\n",
        "&= \\frac{1}{2N} \\sum_{i = 1}^N\\left(\\sum_{j = 1}^D({w_j}x_{i,j}) + b -  y_i\\right)^2 + \\frac{\\alpha}{2N} \\sum_{n=1}^{N} (w_1 + \\cdots + w_n)^2.\n",
        "\\end{align*}\n",
        "where $w_n$ is the $n$th component of the vector $W$ and $x_{i,j}$ is the component in the $i$th row and $j$th column.\n",
        "\n",
        "We're implementing this with gradient descent. Our unknown parameters we want to find are the matrix $W$ and the value $b$, so we'll find the partial derivatives of our loss function with respect to $b$ and with respect to each component of $W$. Let's start with the $w_m$ where $1 \\leq m \\leq D$:\n",
        "\\begin{align*}\n",
        "\\frac{\\partial L}{\\partial w_m} &= \\frac{\\partial}{\\partial w_m}\\left(\\frac{1}{2N} \\sum_{i = 1}^N\\left(\\sum_{j = 1}^D({w_j}x_{i,j}) + b -  y_i\\right)^2 + \\frac{\\alpha}{2N} \\sum_{n=1}^{N} (w_1 + \\cdots + w_n)^2\\right) \\\\\n",
        "&= \\frac{\\partial}{\\partial w_m}\\left(\\frac{1}{2N} \\sum_{i = 1}^N\\left(\\sum_{j = 1}^D({w_j}x_{i,j}) + b -  y_i\\right)^2\\right) + \\frac{\\partial}{\\partial w_m}\\left(\\frac{\\alpha}{2N} \\sum_{n=1}^{N} (w_1 + \\cdots + w_n)^2\\right) \\\\\n",
        "&= \\left(\\frac{1}{2N} \\sum_{i = 1}^N\\frac{\\partial}{\\partial w_m}\\left(\\sum_{j = 1}^D({w_j}x_{i,j}) + b -  y_i\\right)^2\\right) + \\frac{\\partial}{\\partial w_m}\\left(\\frac{\\alpha}{2N} \\sum_{n=1}^{N} (w_1 + \\cdots + w_n)^2\\right) \\\\\n",
        "&= \\frac{1}{N}\\sum_{i = 1}^N\\left(\\sum_{j = 1}^D({w_j}x_{i,j}) + b -  y_i\\right)\\left(\\sum_{j = 1}^D\\frac{\\partial w_j}{\\partial w_m}x_{i,j}\\right) + \\frac{\\alpha}{2N} \\sum_{n=1}^{N} \\frac {\\partial}{\\partial w_m}(w_1 + \\cdots + w_n)^2 \\\\\n",
        "&= \\frac{1}{N}\\sum_{i = 1}^Nx_{i,m}\\left(\\sum_{j = 1}^D({w_j}x_{i,j}) + b -  y_i\\right) + \\frac{\\alpha}{2N} \\sum_{n=1}^{N} \\frac {\\partial}{\\partial w_m}(w_1 + \\cdots + w_n)^2 \\\\\n",
        "&= \\sum_{i = 1}^Nx_{i,m}(\\hat{y}_i - y_i) + \\frac{\\alpha}{N} w_m.\n",
        "\\end{align*}\n",
        "\n",
        "Now, let be the column vector\n",
        "$$\\Delta W = \\left(\\frac{\\partial L}{\\partial w_1}, \\frac{\\partial L}{\\partial w_2}, \\ldots, \\frac{\\partial L}{\\partial w_D} \\right)^T.$$\n",
        "We can rewrite this as\n",
        "$$\\Delta W = \\frac{1}{N}X^T(\\hat{y} - y) + \\frac{\\alpha}{N} W.$$\n",
        "\n",
        "We'll need to do a similar to thing to find $b$ now. Let's start with some derivatives again: \n",
        "\\begin{align*}\n",
        "\\frac{\\partial L}{\\partial b} &= \\frac{\\partial}{\\partial b}\\left(\\frac{1}{2N} \\sum_{i = 1}^N\\left(\\sum_{j = 1}^D({w_j}x_{i,j}) + b -  y_i\\right)^2 + \\frac{\\alpha}{2N} \\sum_{n=1}^{N} (w_1 + \\cdots + w_n)^2 \\right) \\\\\n",
        "&= \\frac{\\partial}{\\partial b}\\left(\\frac{1}{2N} \\sum_{i = 1}^N\\left(\\sum_{j = 1}^D({w_j}x_{i,j}) + b -  y_i\\right)^2 \\right) \\\\\n",
        "&=  \\frac{1}{N} \\sum_{i = 1}^N\\left(\\sum_{j = 1}^D({w_j}x_{i,j}) + b -  y_i\\right) \\\\\n",
        "&= \\frac{1}{N} \\sum_{i = 1}^N (\\hat{y}_i - y).\n",
        "\\end{align*}\n",
        "\n",
        "\n",
        "Since we're implementing gradient descent, we need a rate at which our algorithm \"learns,\" which we'll call $\\beta$. In more mathematical terms, this is just a scaling factor on our parial derivative that we tweak to find our minimum. We'll update our weights and bias in the following way:\n",
        "  $$W_{new} = W_{old} - \\beta\\Delta W,$$\n",
        "  $$b_{new} = b_{old} - \\beta\\dfrac{\\partial{L}}{\\partial{b}}.$$\n",
        "This is starting to look more like something we can code up now!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GFvUVsvknQh",
        "colab_type": "text"
      },
      "source": [
        "# Forward Propagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ref3yx6Hkrmt",
        "colab_type": "text"
      },
      "source": [
        "This is the easy part! We'll start by calculating what our prediction would be:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-u222xWkNI4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward(X, W, b):\n",
        "    y_hat = np.dot(X, W) + b #adds the scalar b to each entry of our vector\n",
        "    return y_hat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0u3-pzTk665",
        "colab_type": "text"
      },
      "source": [
        "# Backward Propagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-XY1Bcpk_qJ",
        "colab_type": "text"
      },
      "source": [
        "So, our initial guess probably isn't the minimzing guess, so what we'll do is update our guess for $W$ and $b$ by using gradient descent:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EULdTBRDlMeJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gradient_descent(W, b, y, y_hat, X, alpha, beta):\n",
        "    z = (y_hat-y)/X.shape[0]\n",
        "    del_b = np.sum(z, axis = 0, keepdims = True)\n",
        "    del_W = np.dot(X.T, z) + alpha/X.shape[0] * W\n",
        "    W = W - beta * del_W\n",
        "    b = b - beta * del_b\n",
        "    return W, b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5Z12s8DmKoR",
        "colab_type": "text"
      },
      "source": [
        "# Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUlNw_hNmPDL",
        "colab_type": "text"
      },
      "source": [
        "Now that we have all the pieces for our algorithm, what's left is to predict after running through enough iterations. We'll take our test data and assume it fits the model. Then we have\n",
        "$$y_{predicted} = X_{test}W_{optimal} + b_{optimal}.$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGm_fkf-mGWT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(W, X_test, b):\n",
        "    y_pred = np.dot(X_test, W) + b #adds the scalar b to each entry of our vector\n",
        "    return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LVh74wchuM8",
        "colab_type": "text"
      },
      "source": [
        "# Checking the Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rt2IlOf7hxJD",
        "colab_type": "text"
      },
      "source": [
        "We need to check if our code is actually minimizing our loss, so let's write a function print out the loss:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGiPmnOeh3QX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_loss(y_hat, y):\n",
        "    return np.linalg.norm(y_hat-y)**2/len(y) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0uyn092m2_n",
        "colab_type": "text"
      },
      "source": [
        "# Putting it Together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-IAESbWm8Ic",
        "colab_type": "text"
      },
      "source": [
        "Okay, so now that we have all the required pieces, let's put it all together. As with any numerical algorithm, let's put this through multiple iterations in a loop and then predict:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlVYFvKnnIwt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "alpha = 10\n",
        "beta = 0.001\n",
        "W = np.zeros(X_train_norm.shape[1], dtype = 'float')\n",
        "y_hat = np.zeros(X_train_norm.shape[0], dtype = 'float')\n",
        "b = 0.0\n",
        "iterations = 100000 #You'll probably need more iterations to get a good result, but google doesn't really give much run time.\n",
        "for n in range(iterations):\n",
        "    y_hat = forward(X_train_norm, W, b)\n",
        "    W, b = gradient_descent(W, b, y_train, y_hat, X_train_norm, alpha, beta)\n",
        "    if(n % 100 == 0):\n",
        "        print(print_loss(y_hat,y_train))\n",
        "pred = predict(W, X_test_norm, b)\n",
        "pred = np.matrix.round(pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fR3pBzo_V2bP",
        "colab_type": "text"
      },
      "source": [
        "# How accurate is This?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3XowRgVn_AG",
        "colab_type": "text"
      },
      "source": [
        "How accurate is this now? Let's check:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ykpun4rnoGPU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3230ef04-344d-412b-8e0c-1e42e7849f66"
      },
      "source": [
        "print(y_test)\n",
        "print(pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[344700. 176500. 270500. ...  62000. 162500. 500001.]\n",
            "[331557. 215508. 276326. ...  87204. 186744. 435574.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kw83_VS-V-JV",
        "colab_type": "text"
      },
      "source": [
        "# Conclusion and Discussions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3TouMedo5jT",
        "colab_type": "text"
      },
      "source": [
        "It looks bad, but this is expected since it's linear regression and it doesn't account for any non-linearity. We could probably feature engineer this to make it a little better, but we probably won't get anything that accurate. We've erased any variables about location since it definitely isn't a variable that scales linearly.  However, location is definitely an important part of housing price. This problem probably won't be solved well using regression, so let's try something a little different in our next notebook."
      ]
    }
  ]
}