{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic_Regression.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOlNdMSu9c1+QufUgQuIpPD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ilovely11/ly11/blob/master/Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXzdjG8P6bQ7",
        "colab_type": "text"
      },
      "source": [
        "# Logisitic Regression\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yByX_GQN6a_b",
        "colab_type": "text"
      },
      "source": [
        "This is a tutorial for a Logistic Regresion algoirthm from scratch. We'll use the iris dataset since it's a simple data set to test our algorithm on. We've used elements of a tutorial from Guowei Wei's Machine Learning class at MSU, but we've added a slight mathematical twist to our work rahter than trying to fit dimensions together."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-bcYL1v1wQ4",
        "colab_type": "text"
      },
      "source": [
        "#Importing Data for Usage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyycMGuBArmD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvkiztIfkI4t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "iris = datasets.load_iris()\n",
        "X = iris.data[:, :2]  # we only take the first two features.\n",
        "y = iris.target\n",
        "for n in range(len(y)):\n",
        "    if (y[n] != 0):\n",
        "        y[n] = 1\n",
        "#Making our data set into a binary classification problem.  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpDklNjIk00g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntRKjeaf14AJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaler = StandardScaler() # call an object function\n",
        "scaler.fit(X_train)   # calculate mean\n",
        "X_train_norm = scaler.transform(X_train)  # apply normalization on X_train\n",
        "X_test_norm = scaler.transform(X_test)    # apply normalization on X_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXcB7_yB17Sc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "ed2a7bd5-6061-4036-b7cc-c85902d027d4"
      },
      "source": [
        "print(X_train_norm.shape)\n",
        "print(X_test_norm.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(100, 2)\n",
            "(50, 2)\n",
            "(100,)\n",
            "(50,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4ifLgXM6d2_",
        "colab_type": "text"
      },
      "source": [
        "#Sigmoid Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8DdXjyDmgQ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(x):\n",
        "    return 1/(1+np.exp(-x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKKjT1v_6e4y",
        "colab_type": "text"
      },
      "source": [
        "#Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiAD9RGppR8M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(predicted, actual):\n",
        "    diff = predicted - actual\n",
        "    return 1.0 - (float(np.count_nonzero(diff)) / len(diff))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtX7DWDy08lX",
        "colab_type": "text"
      },
      "source": [
        "#Forward Propagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-ibyf9oh0i6",
        "colab_type": "text"
      },
      "source": [
        "The structure of our network is the following:\n",
        "$$X \\xrightarrow{\\text{linear map}} f \\xrightarrow{\\text{sigmoid}} \\hat{y}$$\n",
        "Assume that our dataset has $N$ datapoints and $D$ features. Then our matrix $X$ is $N \\times D$, our weight matrix $W$ is $D \\times 1$, and our output is $N \\times 1$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Uf8kY8rkHV3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward(X, W, b):\n",
        "  f = np.dot(X,W) + b\n",
        "  y_hat = sigmoid(f)\n",
        "  return y_hat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rjf0ShUo1ANZ",
        "colab_type": "text"
      },
      "source": [
        "#Backpropagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54-mnMRqgKOd",
        "colab_type": "text"
      },
      "source": [
        "Let's go through the backpropagation process now. So, like before, we're going to take derivatives with respect to our weights. Let's start with the simple case of finding the partial derivatives with respect to each component of $\\hat{y}$. Let $1 \\leq m \\leq N$. Then\n",
        "\\begin{align*}\n",
        "\\frac{\\partial L}{\\partial \\hat{y}_m} &=  -\\frac{1}{N} \\sum_{n=1}^N y \\frac{\\partial}{\\partial \\hat{y}_m}\\log(\\hat{y}_n) + (1-y)\\frac{\\partial}{\\partial \\hat{y}_m}\\log(1-\\hat{y}_n) \\\\\n",
        "&= -\\frac{1}{N}\\left( \\frac{y}{\\hat{y}_m} - \\frac{1-y}{1-\\hat{y}_m}\\right) \\\\\n",
        "&= \\frac{1}{N}\\left(\\frac{1-y}{1-\\hat{y}_m} -\\frac{y}{\\hat{y}_m}\\right).\n",
        "\\end{align*}\n",
        "By the chain rule, we have\n",
        "$$\\frac{\\partial L}{\\partial f_i} = \\sum_{m=1}^N \\frac{\\partial L}{\\partial \\hat{y}_m} \\frac{\\partial \\hat{y}_m}{\\partial f_i}.$$\n",
        "Now, notice that\n",
        "$$\n",
        "\\frac{\\partial \\hat{y}_m}{\\partial f_i} = \n",
        "\\begin{cases}\n",
        "\\hat{y}_m(1-\\hat{y}_m), \\quad m = i \\\\\n",
        "0. \\qquad m \\neq i\n",
        "\\end{cases}$$\n",
        "When we substiutte this in, we get\n",
        "$$\\frac{\\partial L}{\\partial f_i} = \\frac{1}{N}\\left(\\frac{1-y}{1-\\hat{y}_m} -\\frac{y}{\\hat{y}_m}\\right) \\hat{y}_m(1-\\hat{y}_m) = \\frac{1}{N}(1-y)\\hat{y}_m - y(1-\\hat{y}) = \\frac{1}{N}(\\hat{y}_m - y).$$\n",
        "Okay, final step now. Let's find\n",
        "$$\\frac{\\partial L}{\\partial W_i} =  \\sum_{m=1}^N \\frac{\\partial L}{\\partial f_m} \\frac{\\partial f_m}{\\partial W_i}.$$\n",
        "Write\n",
        "$$f_m = \\sum_{n=1}^N x_{m,n} W_n + b.$$\n",
        "Then \n",
        "$$\n",
        "\\frac{\\partial f_m}{\\partial W_i} = \n",
        "\\begin{cases}\n",
        "x_{m,n}, \\quad n = i \\\\\n",
        "0. \\qquad n \\neq i\n",
        "\\end{cases}$$\n",
        "When we substitute this in, we get\n",
        "$$\\Delta W = \\frac{\\partial L}{\\partial W_i} = \\frac{1}{N}\\sum_{m=1}^Nx_{m,i}(\\hat{y}_m - y).$$\n",
        "Let's now put these into a vector as $\\left(L_{w_1}, L_{w_2}, \\ldots, L_{w_D}\\right)^{T}$. To finish up, we see that we can vectorize this as\n",
        "$$\\Delta W = X^{T}(\\hat{y}- y).$$\n",
        "Performing a similar calculation for the derivative with respect to the bias, we get\n",
        "$$\\frac{\\partial L}{\\partial b} = \\frac{1}{N}\\sum_{m=1}^N(\\hat{y}_m - y).$$\n",
        "Remember that our update method is \n",
        "  $$W_{new} = W_{old} - \\beta\\Delta W,$$\n",
        "  $$b_{new} = b_{old} - \\beta\\dfrac{\\partial{L}}{\\partial{b}}.$$\n",
        "where $\\beta$ is our learning rate. See the code below for the implementation.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYjo7ya4m0eU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def backpropagation(X, W, b, y_hat, y, beta):\n",
        "    d = (sigmoid(y_hat) - y) / X.shape[0]\n",
        "    dW = np.dot(X.T, d)\n",
        "    db = np.sum(d)\n",
        "    W = W - beta * dW\n",
        "    b = b - beta * db\n",
        "    return W, b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9u-gD2Zp1EXn",
        "colab_type": "text"
      },
      "source": [
        "#Loss Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAu4XYJdgHuQ",
        "colab_type": "text"
      },
      "source": [
        "Our loss function will be the same as usual for classification, the cross entropy loss: \n",
        "$$L(\\hat{y}) = -\\frac{1}{N} \\sum_{n=1}^N y \\log(\\hat{y}_n) + (1-y)\\log(1-\\hat{y}_n).$$\n",
        "Pretty easy to handle for the most part. In this case, we won't add a regularization parameter to prevent overfitting. This can be easily added by modifying two or three lines of code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qk1zZa14ng2w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_loss(y, yhat): \n",
        "    loss = -1/y.shape[0] * np.sum(y * np.log(y_hat) + (1 - y) * np.log(1-y_hat))\n",
        "    print(loss)\n",
        "    return "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VPpbxJW1Dx4",
        "colab_type": "text"
      },
      "source": [
        "#Making A Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LG6LWVVrpEg_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(X, W, b):\n",
        "    y_pred = np.dot(X,W) + b\n",
        "    y_pred = sigmoid(y_pred)\n",
        "    for n in range(len(y_pred)):\n",
        "        if (y_pred[n] < 0.5):\n",
        "            y_pred[n] = 0\n",
        "        else:\n",
        "            y_pred[n] = 1\n",
        "    return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MwASR5N1I6p",
        "colab_type": "text"
      },
      "source": [
        "#Putting Everything Together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTfN48suprqN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "7a8a0be4-2090-4e15-d000-5692017f8ed3"
      },
      "source": [
        "W = np.zeros(X.shape[1], dtype = 'float')\n",
        "y_hat = np.zeros(X.shape[0], dtype = 'float')\n",
        "y_pred = np.zeros(y_test.shape[0], dtype = 'int')\n",
        "b = 0\n",
        "beta = 0.001\n",
        "iterations = 10000\n",
        "for n in range(iterations):\n",
        "    y_hat = forward(X_train_norm, W, b)\n",
        "    #print_loss(y_hat, y_train)\n",
        "    W, b = backpropagation(X_train_norm, W, b, y_hat, y_train, beta)\n",
        "y_pred = predict(X_test_norm, W, b)\n",
        "acc = accuracy(y_test, y_pred)\n",
        "print(y_test)\n",
        "print(y_pred)\n",
        "print('The accuracy of this model is: %f percent' %(acc*100))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-7239affc1189>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'float'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'float'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7GEETOwgc39",
        "colab_type": "text"
      },
      "source": [
        "Okay, not that bad, but not perfect. What about sklearn?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKoBwL6FA8w6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ee8a3c13-4108-44d3-8433-96e846e7ab58"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "clf = LogisticRegression(random_state=0).fit(X, y)\n",
        "clf.predict(X[:2, :])\n",
        "clf.predict_proba(X[:2, :])\n",
        "clf.score(X, y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_F5r-vhTghCr",
        "colab_type": "text"
      },
      "source": [
        "Looks like we can improve this slightly with some parameter tuning, like adding a regularization term."
      ]
    }
  ]
}